name: crawl cumulative data
on:
  schedule:
    - cron: "*/30 * * * *"
jobs:
  runCrawler:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python 3.x
        uses: actions/setup-python@v1
        with:
          python-version: "3.x"
          architecture: "x64"
      # - name: Cache dependencies
      #   uses: actions/cache@v2
      #   id: cache
      #   with:
      #     path: ~/.cache/pip
      #     key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
      #     restore-keys: |
      #       ${{ runner.os }}-pip-
      - name: Install dependencies
        # if: steps.cache.outputs.cache-hit != 'true'
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Run crawler
        run: |
          python "data/crawlKoreaTotalCumulativeData.py"
          python "data/crawlWorldCumulativeData.py"
      - name: Commit files
        run: |
          if [[ -n $(git status --porcelain) ]]; then
              git config --local user.email "commit-bot@taeuk.me"
              git config --local user.name "LiveCoronaBot"
              git add .
              git commit -m "Run crawler and update cumulative data"
          fi
      - name: Push changes
        uses: ad-m/github-push-action@master
        with:
          branch: "master"
          github_token: ${{ secrets.GITHUB_TOKEN }}
