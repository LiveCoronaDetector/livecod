name: Run cumulative data crawler
on:
  schedule:
    - cron:  '*/30 * * * *'
jobs:
  runCrawler:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python 3.x
        uses: actions/setup-python@v1
        with:
          python-version: '3.x'
          architecture: 'x64'
      - uses: actions/cache@v1
        id: cache
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      - name: Install dependencies
        if: steps.cache.outputs.cache-hit != 'true'
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Run crawler
        run: |
          python "data/crawlKoreaTotalCumulativeData.py"
          python "data/crawlKoreaRegionalCumulativeData.py"
          python "data/crawlWorldCumulativeData.py"
      - name: Commit files
        run: |
          if [[ -n $(git status --porcelain) ]]; then
              git config --local user.email "commit-bot@taeuk.me"
              git config --local user.name "LiveCoronaBot"
              git add .
              git commit -m "Run crawler and update cumulative data"
          fi
      - name: Push changes
        uses: ad-m/github-push-action@master
        with:
          branch: 'master'
          github_token: ${{ secrets.GITHUB_TOKEN }}
